{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dwFndmJZJytJ"
   },
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install -q gradio seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yz8OtP0OJ-OR"
   },
   "outputs": [],
   "source": [
    "#Importing Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Rescaling, GlobalAveragePooling2D, Input, Dropout, Dense\n",
    "from tensorflow.keras import layers, optimizers\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras.applications import EfficientNetV2B2\n",
    "import gradio as gr\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rW97XcQYKDe7"
   },
   "outputs": [],
   "source": [
    "# Dataset path and parameters\n",
    "dataset_dir = \"/content/drive/MyDrive/TrashType_Image_Dataset (1)\"\n",
    "image_size = (124, 124)\n",
    "batch_size = 32\n",
    "seed = 42\n",
    "\n",
    "# âœ… Load training and validation sets\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    dataset_dir, validation_split=0.2, subset=\"training\", seed=seed,\n",
    "    shuffle=True, image_size=image_size, batch_size=batch_size\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    dataset_dir, validation_split=0.2, subset=\"validation\", seed=seed,\n",
    "    shuffle=True, image_size=image_size, batch_size=batch_size\n",
    ")\n",
    "\n",
    "class_names = train_ds.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1_HRIu2KFbp"
   },
   "outputs": [],
   "source": [
    "# Create test set from validation set\n",
    "val_batches = tf.data.experimental.cardinality(val_ds)\n",
    "test_ds = val_ds.take(val_batches // 2)\n",
    "val_dat = val_ds.skip(val_batches // 2)\n",
    "test_ds_eval = test_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Distribution function\n",
    "def count_distribution(dataset, class_names):\n",
    "    total = 0\n",
    "    counts = {name: 0 for name in class_names}\n",
    "    for _, labels in dataset:\n",
    "        for label in labels.numpy():\n",
    "            counts[class_names[label]] += 1\n",
    "            total += 1\n",
    "    if total == 0:  # Handle empty dataset\n",
    "        return {name: 0.0 for name in class_names}\n",
    "    for k in counts:\n",
    "        counts[k] = round((counts[k] / total) * 100, 2)\n",
    "    return counts\n",
    "\n",
    "# Bar chart\n",
    "def simple_bar_plot(dist, title):\n",
    "    plt.bar(dist.keys(), dist.values(), color='cornflowerblue')\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Percentage (%)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylim(0, 100)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot distributions\n",
    "train_dist = count_distribution(train_ds, class_names)\n",
    "val_dist = count_distribution(val_ds, class_names)\n",
    "test_dist = count_distribution(test_ds, class_names)\n",
    "overall_dist = {k: round((train_dist[k] + val_dist[k]) / 2, 2) for k in class_names}\n",
    "\n",
    "simple_bar_plot(train_dist, \"Training Set Distribution (%)\")\n",
    "simple_bar_plot(val_dist, \"Validation Set Distribution (%)\")\n",
    "simple_bar_plot(test_dist, \"Test Set Distribution (%)\")\n",
    "simple_bar_plot(overall_dist, \"Overall Class Distribution (%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XiP8yi0WKLzw"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JgG-cAp4KHZS"
   },
   "outputs": [],
   "source": [
    "# Show sample images\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "    for i in range(12):\n",
    "        ax = plt.subplot(4, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9NH9bzeKKN0Z"
   },
   "outputs": [],
   "source": [
    "# Compute class weights\n",
    "class_counts = {i: 0 for i in range(len(class_names))}\n",
    "all_labels = []\n",
    "for _, labels in train_ds:\n",
    "    for label in labels.numpy():\n",
    "        class_counts[label] += 1\n",
    "        all_labels.append(label)\n",
    "\n",
    "# Get unique labels present in the training data\n",
    "unique_labels = np.unique(all_labels)\n",
    "\n",
    "class_weights_array = compute_class_weight('balanced',\n",
    "                                           classes=unique_labels,\n",
    "                                           y=all_labels)\n",
    "\n",
    "# Create a dictionary with weights for all class indices\n",
    "class_weights = {i: 0.0 for i in range(len(class_names))}\n",
    "for i, label in enumerate(unique_labels):\n",
    "    class_weights[label] = class_weights_array[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KNLMtEbkKPxd"
   },
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "data_augmentation = Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.1),\n",
    "    layers.RandomContrast(0.1),\n",
    "])\n",
    "\n",
    "# Base Model\n",
    "base_model = EfficientNetV2B2(include_top=False, input_shape=(124, 124, 3),\n",
    "                              include_preprocessing=True, weights='imagenet')\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:100]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Final Model\n",
    "model = Sequential([\n",
    "    Input(shape=(124, 124, 3)),\n",
    "    data_augmentation,\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dropout(0.3),\n",
    "    Dense(len(class_names), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=1e-4),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g5uA6lXhKRJV"
   },
   "outputs": [],
   "source": [
    "# Early stopping\n",
    "early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train\n",
    "history = model.fit(train_ds, validation_data=val_ds, epochs=15,\n",
    "                    class_weight=class_weights, callbacks=[early])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KmDktFIfKUN4"
   },
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(len(acc))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Train Acc')\n",
    "plt.plot(epochs_range, val_acc, label='Val Acc')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Train Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Val Loss')\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cXDvLD-eKTmT"
   },
   "outputs": [],
   "source": [
    "# Evaluate with confusion matrix\n",
    "y_true = np.concatenate([y.numpy() for _, y in val_ds])\n",
    "y_pred_probs = model.predict(val_ds)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(classification_report(y_true, y_pred, target_names=class_names, labels=np.arange(len(class_names))))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names, cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NTdzFa0RKWTs"
   },
   "outputs": [],
   "source": [
    "# Show predictions\n",
    "for images, labels in test_ds_eval.take(1):\n",
    "    predictions = model.predict(images)\n",
    "    pred_labels = tf.argmax(predictions, axis=1)\n",
    "    for i in range(8):\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(f\"True: {class_names[labels[i]]}, Pred: {class_names[pred_labels[i]]}\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uEz5ZSpVKb9X"
   },
   "outputs": [],
   "source": [
    "# Save\n",
    "model.save('efficientnetv2b2.keras')\n",
    "\n",
    "# Load\n",
    "model = tf.keras.models.load_model('efficientnetv2b2.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dDul87ihKeFV"
   },
   "outputs": [],
   "source": [
    "# Gradio Inference Function\n",
    "from tensorflow.keras.applications.efficientnet_v2 import preprocess_input\n",
    "\n",
    "def classify_image(img):\n",
    "    img = img.resize((124, 124))\n",
    "    img_array = np.array(img, dtype=np.float32)\n",
    "    img_array = preprocess_input(img_array)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    prediction = model.predict(img_array)\n",
    "    pred_index = np.argmax(prediction)\n",
    "    pred_name = class_names[pred_index]\n",
    "    confidence = prediction[0][pred_index]\n",
    "    return f\"Predicted: {pred_name} (Confidence: {confidence:.2f})\"\n",
    "\n",
    "# Interface\n",
    "iface = gr.Interface(\n",
    "    fn=classify_image,\n",
    "    inputs=gr.Image(type=\"pil\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Garbage Classification using EfficientNetV2B2\",\n",
    "    description=\"Upload an image of garbage and the model will classify it.\"\n",
    ")\n",
    "iface.launch()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMLCOcyImyhAI4Ifh1+Dx59",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
